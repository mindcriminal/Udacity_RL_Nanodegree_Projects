{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4680d1-2c2f-4878-9867-e25ca437b126",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration/Competition Project Report\n",
    "\n",
    "This project builds on the DDPG Pendulum solution provided as part of Udacity's \"deep-reinforcement-learning-master\" Github repo and extends it to use multiple agents\n",
    "* [DDPG Pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)\n",
    "\n",
    "While the pendulum solution interacted with an OpenAI Gym environment, this project replaced that environment with Unity's Tennis environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b1b86-ff06-4f3c-a50e-9ebc8b626621",
   "metadata": {},
   "source": [
    "# Learning Algorithm\n",
    "\n",
    "A Pytorch Deep Deterministic Policy Gradient (DDPG) actor/critic approach was trained to solve the environment with two agents.\n",
    "\n",
    "In PyTorch, the neural network layers are defined in the init function and the forward pass is defined in the forward function, which is invoked automatically when the class is called.\n",
    "\n",
    "Two networks were defined, namely an Actor and a Critic.\n",
    "\n",
    "The Actor maps states to a deterministic action. There are two hidden layers (128 fully connected nodes). The output activation function is tanh as the output values vary between -1 and 1.\n",
    "\n",
    "The Critic takes the Actor's action and uses it for training (used in action value function). There are two hidden layers (128 fully connected nodes). The output activation function is ReLU as the output values needs to be positive.\n",
    "\n",
    "My original implementation resulted in unstable/ineffective early training which eventually converged in 1551 episodes. I then implemented Batch Normalization which is a method used to make training of neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling values. With Batch Normalization in place after layer activation, the environment was solved in just 700 episodes.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "An agent class is defined that interacts with the environment and updates both networks. This environment includes 2 agents.\n",
    "\n",
    "Noise was used to balance exploration vs. exploitation when selecting each of the agents' action values.  A discount factor of 0.9 was used and a learning rate of 1e-3 was used for both networks.\n",
    "\n",
    "Experiences from each agent are shared and added to the replay memory as actions are taken. Every time step, the network will sample actions from this replay memory as long as the size of the memory has reached 256 samples. These samples will be used to update the network. A soft update startegy is used (using hyperparameter TAU) to slowly blend the regular network weights (training network) with the target network weights (the one we are using for prediction to stabilize training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c014d-7126-41f4-9e07-002d027082bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Plot of Rewards\n",
    "\n",
    "## Without Batch Normalization\n",
    "![Scores Plot](./Scores_Per_Episode_No_Batch_Norm.png)\n",
    "\n",
    "## With Batch Normalization\n",
    "![Scores Plot](./Scores_Per_Episode.png)\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents).\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5. My environment was solved in 1551 episodes without Batch Normalization and in 700 episodes with Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44657e-9ebc-49da-9a4b-bed8249eb55f",
   "metadata": {},
   "source": [
    "# Ideas For Future Work\n",
    "\n",
    "Future work can consider the use of different combinations of Value and Policy Based Methods for continuous control tasks such as A2C or A3C.  Hyperparameter sweeps could also be used to improve the stability and speed of the training the standard DDPG used here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl37_anaconda",
   "language": "python",
   "name": "rl37_anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4680d1-2c2f-4878-9867-e25ca437b126",
   "metadata": {},
   "source": [
    "# Policy Based Methods Continuous Control Project Report\n",
    "\n",
    "This project builds on the DDPG Pendulum solution provided as part of Udacity's \"deep-reinforcement-learning-master\" Github repo\n",
    "* [DDPG Pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)\n",
    "\n",
    "While the pendulum solution interacted with an OpenAI Gym environment, this project replaced that environment with Unity's Reacher environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b1b86-ff06-4f3c-a50e-9ebc8b626621",
   "metadata": {},
   "source": [
    "# Learning Algorithm\n",
    "\n",
    "A Pytorch Deep Deterministic Policy Gradient (DDPG) actor/critic approach was trained to solve the environment.\n",
    "\n",
    "In PyTorch, the neural network layers are defined in the init function and the forward pass is defined in the forward function, which is invoked automatically when the class is called.\n",
    "\n",
    "Two networks were defined, namely an Actor and a Critic.\n",
    "\n",
    "The Actor maps states to a deterministic action. There are two hidden layers (128 fully connected nodes). The output activation function is tanh as the output values vary between -1 and 1.\n",
    "\n",
    "The Critic takes the Actor's action and uses it for training (used in action value function). There are two hidden layers (128 fully connected nodes). The output activation function is ReLU as the output values needs to be positive.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector must be a number between -1 and 1.\n",
    "\n",
    "An agent is defined that interacts with the environment and updates both networks.\n",
    "\n",
    "Noise was used to balance exploration vs. exploitation when selecting action values.  A discount factor of 0.9 was used and a learning rate of 1e-3 was used for both networks.\n",
    "\n",
    "Every time step, the network will sample actions from replay memory as long as the size of the memory has reached 256 samples. These samples will be used to update the network. A soft update startegy is used (using hyperparameter TAU) to slowly blend the regular network weights (training network) with the target network weights (the one we are using for prediction to stabilize training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c014d-7126-41f4-9e07-002d027082bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Plot of Rewards\n",
    "\n",
    "![Scores Plot](./Scores_Per_Episode.png)\n",
    "\n",
    "The environment is considered solved when the average reward for an episode reaches 30 over 100 episodes. My environment was solved in 182 episodes with an Average Score value of 30.20. This was very difficult to achieve. Hours were spent tweaking network structures and hyperparameters. There were a number of different combinations of hyperparameters that led to very promising training just to have it break down after hitting a peak as shown below. \n",
    "\n",
    "![Training Breakdown](./Training_Breakdown.png)\n",
    "\n",
    "In these runs, the results per episode were varying wildly and the networks were generally unstable. Tweaking gamma values, making noisy updates less noisy (reducing sigma), and using less nodes in my neural net made this solution finally converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44657e-9ebc-49da-9a4b-bed8249eb55f",
   "metadata": {},
   "source": [
    "# Ideas For Future Work\n",
    "\n",
    "Future work can consider the use of different Policy Based Methods for continuous control tasks such as REINFORCE, TNPG, RWR, REPS, TRPO, CEM and CMA-ES.  Hyperparameter sweeps could also be used to improve the stability and speed of the training the standard DDPG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl37_anaconda",
   "language": "python",
   "name": "rl37_anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4680d1-2c2f-4878-9867-e25ca437b126",
   "metadata": {},
   "source": [
    "# Value Based Methods Navigation Project Report\n",
    "\n",
    "This project builds on the Deep Q-Network solution provided as part of Udacity's 'Value-based-methods-main' Github repo\n",
    "* [Deep Q-Network](https://github.com/udacity/Value-based-methods/tree/main/dqn)\n",
    "\n",
    "While this solution interacted with an OpenAI Gym environment, this project replaced that environment with Unity's Bananna Collector environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b1b86-ff06-4f3c-a50e-9ebc8b626621",
   "metadata": {},
   "source": [
    "# Learning Algorithm\n",
    "\n",
    "A Pytorch Deep Q-Network (DQN) using replay memory was trained to solve the environment.\n",
    "\n",
    "In PyTorch, the neural network layers are defined in the init function and the forward pass is defined in the forward function, which is invoked automatically when the class is called.\n",
    "\n",
    "3 Linear layers were used with a Relu activation function.  The first layer takes the input state_size (37) and is fully connected to 64 nodes.  The second layer takes those 64 nodes and is fully connected to another 64 nodes.  The third layer takes 64 nodes and is fully connected to the output action_size (4).\n",
    "\n",
    "An agent is defined that interacts with the environment and updates the DQN.\n",
    "\n",
    "An epsilon value of 1.0 with a decay rate of 0.005 was used.  A random draw again epsilon was used to balance exploration vs. exploitation when selecting action values.  A discount factor of 0.99 was used and a learning rate of 5e-4 was used.  \n",
    "\n",
    "Every 4 time steps, the network will sample actions from replay memory as long as the size of the memory has reached 32 samples. These samples will be used to update the network (weights from local copied to target) for use when the agent needs to take the next action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c014d-7126-41f4-9e07-002d027082bd",
   "metadata": {},
   "source": [
    "# Plot of Rewards\n",
    "\n",
    "![Scores Plot](./Scores_Per_Episode.png)\n",
    "\n",
    "The environment is considered solved when the average reward for an episode reaches 13.00.  My environment was solved in 516 episodes with an Average Score value of 13.03.  Max scores per episode were considerbly higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44657e-9ebc-49da-9a4b-bed8249eb55f",
   "metadata": {},
   "source": [
    "# Ideas For Future Work\n",
    "\n",
    "Several improvements to a standard DQN with Replay Memory have been suggested in the coursework, namely Double DQNs, Dueling DQNs and replacing Replay Memory with Prioritized Replay Memory.  A hyperparameter sweep could also be used to improve the standard DQN as well.\n",
    "\n",
    "It is also important to understand the definition of done for this environment that is not specified anywhere.  I could not find any information on when/why the done flag gets set in the environment.  This information might help with hyperparameter selection in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl37_anaconda",
   "language": "python",
   "name": "rl37_anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
